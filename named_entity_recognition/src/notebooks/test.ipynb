{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import Markdown, display\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from core.dataset.token_classification_datamodule import LayoutLMDataset\n",
    "from core.model.liltxlm_ner_model import LiltXLMNer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_bbox(bbox_normed, width, height):\n",
    "    return [\n",
    "        int((bbox_normed[0] * width) / 1000.0),\n",
    "        int((bbox_normed[1] * height) / 1000.0),\n",
    "        int((bbox_normed[2] * width) / 1000.0),\n",
    "        int((bbox_normed[3] * height) / 1000.0)     \n",
    "    ]\n",
    "\n",
    "def infer(inputs, model : LiltXLMNer, tokenizer : AutoTokenizer):\n",
    "\n",
    "    # Must add a batch dimension for the model to work\n",
    "    inputs['input_ids'] = inputs['input_ids'].unsqueeze(0)  \n",
    "    inputs['bbox'] = inputs['bbox'].unsqueeze(0)\n",
    "    inputs['attention_mask'] = inputs['attention_mask'].unsqueeze(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "\n",
    "    # Get the predicted labels as the argmax across the last dimension\n",
    "    # TODO: I should also keep the logits for the loss function and apply softmax\n",
    "    # so that I can get the probabilities (confidence values) for each class\n",
    "    preds = torch.argmax(output.logits, dim=2)      \n",
    "\n",
    "    # Create a list of tuples\n",
    "    results = []\n",
    "    for i in range(preds.shape[0]):\n",
    "        word = inputs[\"input_ids\"][i].squeeze().tolist()\n",
    "        bbox = inputs[\"bbox\"][i].squeeze().tolist()\n",
    "        label = preds[i].squeeze().tolist()\n",
    "        results.append((word, bbox, label))\n",
    "        \n",
    "    return results      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "    1: \"red\",\n",
    "    2: \"green\",\n",
    "    3: \"blue\",\n",
    "    4: \"yellow\",\n",
    "    5: \"purple\",\n",
    "    6: \"orange\",\n",
    "    7: \"cyan\",\n",
    "    8: \"magenta\",\n",
    "    9: \"brown\",\n",
    "    10: \"lime\",\n",
    "    11: \"pink\",\n",
    "    12: \"gray\",\n",
    "    13: \"olive\",\n",
    "    14: \"teal\",\n",
    "}\n",
    "\n",
    "def show_image_with_bboxes(output, image):\n",
    "    # Take the image and draw the bounding boxes use PIL\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "    for words, bboxs, labels in output:\n",
    "        for word, bbox, label in zip(words, bboxs, labels):\n",
    "            bbox = denormalize_bbox(bbox, image.width, image.height)\n",
    "            if label in color_dict:\n",
    "                draw.rectangle(bbox, outline=color_dict[label], width=3)              \n",
    "\n",
    "    # Display the image use matplotlib\n",
    "    plt.figure(figsize=(18, 14))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Program Files\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Program Files/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "csv_dir = r'YOUR_CSV_DIR'\n",
    "image_dir = r'YOUR_IMAGE_DIR'\n",
    "\n",
    "csv_files = glob.glob(csv_dir + '/*.csv')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nielsr/lilt-xlm-roberta-base\")\n",
    "label2idx = {\n",
    "    # YOUR LABELS\n",
    "}\n",
    "dataset = LayoutLMDataset(csv_dir=None, image_dir=image_dir, csv_files=csv_files, tokenizer=tokenizer, label2idx=label2idx, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512, 4])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[100][0]['input_ids'].shape)\n",
    "print(dataset[100][0]['bbox'].shape)\n",
    "print(dataset[100][0]['attention_mask'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LiltForTokenClassification were not initialized from the model checkpoint at nielsr/lilt-xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LiltXLMNer.load_from_checkpoint(\n",
    "    r\"MODEL_PATH\",\n",
    "    num_labels=NUM_LABELS,\n",
    "    learning_rate=5e-5,\n",
    "    label2idx=label2idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bd5187d5e14ff1a900c74f483b2d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=12055, description='index', max=24110), Output()), _dom_classes=('widgetâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(index=(0, len(dataset)-1))\n",
    "def show_image(index):\n",
    "    item, image = dataset[index]\n",
    "    output = infer(item, model, tokenizer)\n",
    "    show_image_with_bboxes(output, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "typhon-lm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
